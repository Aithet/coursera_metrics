\documentclass[russian,ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme{Madrid}
\usecolortheme{whale}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=russian]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{}
\fi
\newif\ifbibliography

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \let\insertsectionnumber\relax
    \let\sectionname\relax
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}

\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
\author[Эконометрика. Лекция 4]{Эконометрика. Лекция 4}
\newcommand{\e}{\varepsilon}

\title{Мультиколлинеарность}
\date{}

\begin{document}
\frame{\titlepage}

\begin{frame}{Определение мультиколлинеарности}

Мультиколлинеарность --- наличие линейной зависимости между регрессорами

\begin{itemize}
\tightlist
\item
  строгая мультиколлинеарность --- идеальная линейная зависимость
\item
  нестрогая мультиколлинеарность --- примерная линейная зависимость
\end{itemize}

\end{frame}

\begin{frame}{Строгая мультиколлинеарность}

Строгая мультиколлинеарность --- идеальная линейная зависимость между
регрессорами.

Пример:

\[
X=\begin{pmatrix}
1 & 4 & 12 & 8 \\ 
1 & 3 & 3 & 3 \\ 
1 & 1 & 7 & 4 \\ 
1 & 2 & 4 & 3 \\
1 & 3 & 5 & 4 \\
\vdots & \vdots & \vdots & \vdots
\end{pmatrix} 
\]

Здесь: \(x_{\cdot 2}+x_{\cdot 3}=2x_{\cdot 4}\)

\end{frame}

\begin{frame}{Строгая мультиколлинеарность}

Частая причина: неправильно включены дамми-переменные

Пример с ошибкой:

\[
wage_i=\beta_1 + \beta_2 male_i + \beta_3 female_i + \beta_4 educ_i + \varepsilon_i
\]

Здесь: \(x_{\cdot 1}=x_{\cdot 2}+x_{\cdot 3}\)

\[
X=\begin{pmatrix}
1 & 1 & 0 & 16 \\ 
1 & 1 & 0 & 11 \\ 
1 & 0 & 1 & 18 \\ 
1 & 1 & 0 & 10 \\ 
\vdots & \vdots & \vdots & \vdots
\end{pmatrix} 
\]

\end{frame}

\begin{frame}{Последствия строгой мультиколлинеарности}

В теории: оценки МНК неединственны.

Данные три модели эквивалентны:

\[
\widehat{wage}_i=15 + 3 male_i -2 female_i + 3 educ_i 
\]

\[
\widehat{wage}_i=28 -10 male_i -15 female_i + 3 educ_i 
\]

\[
\widehat{wage}_i=18 + 0 male_i -5 female_i + 3 educ_i 
\]

\end{frame}

\begin{frame}{Строгая мультиколлинеарность на практике}

Если попросить компьютерный пакет оценить регрессию со строгой
мультиколлинеарностью, то пакет может:

\begin{itemize}
\tightlist
\item
  выдать сообщение об ошибке
\item
  автоматически удалить переменную {[}R{]}
\end{itemize}

\end{frame}

\begin{frame}{Нестрогая мультиколлинеарность}

Нестрогая мультиколлинеарность --- примерная линейная зависимость между
регрессорами

Причины:

\begin{itemize}
\tightlist
\item
  регрессоры, измеряющие примерно одно и то же: валютный курс на начало
  и на конец дня
\item
  естественные соотношения между регрессорами: возраст, стаж и
  количество лет обучения
\end{itemize}

\end{frame}

\begin{frame}{Последствия нестрогой мультиколлинеарности}

Нестрогая мультиколлинеарность НЕ нарушает стандартный набор предпосылок

Оценки \(\hat{\beta}_j\) несмещенные, асимптотически нормальные, можно
проверять гипотезы и строить доверительные интервалы

\end{frame}

\begin{frame}{Последствия нестрогой мультиколлинеарности}

Хотя бы один из регрессоров хорошо объясняется другими регрессорами

\[
se^2(\hat{\beta}_j)=\frac{\hat{\sigma}^2}{RSS_j}=\frac{\hat{\sigma}^2}{TSS_j\cdot (1-R^2_j)}=
\frac{1}{1-R^2_j}\frac{\hat{\sigma}^2}{TSS_j}
\]

Высокие стандартные ошибки \(se(\hat{\beta}_j)\)

\end{frame}

\begin{frame}{Неприятные проявления высоких стандартных ошибок}

\begin{itemize}
\tightlist
\item
  очень широкие доверительные интервалы
\item
  незначимые коэффициенты
\item
  чувствительность модели к добавлению/удалению наблюдения
\end{itemize}

\end{frame}

\begin{frame}{Практический признак нестрогой мультиколлинеарности}

На практике мультиколлинеарность можно заметить, если:

\begin{itemize}
\item
  Несколько коэффициентов незначимы по отдельности
\item
  Гипотеза об их одновременном равенстве нулю отвергается
\end{itemize}

\end{frame}

\begin{frame}{Количественные признаки мультиколлинеарности}

\begin{itemize}
\tightlist
\item
  коэффициент вздутия дисперсии (Variance Inflation Factor)
\end{itemize}

\[
VIF_j=\frac{1}{1-R^2_j}
\]

\[
se^2(\hat{\beta}_j)=VIF_j \cdot \frac{\hat{\sigma}^2}{TSS_j}
\]

\begin{itemize}
\tightlist
\item
  выборочные корреляции между регрессорами
\end{itemize}

Некоторые источники: \(VIF_j > 10\), \(sCorr(x,z)>0.9\)

\end{frame}

\begin{frame}{Что делать?}

\begin{itemize}
\tightlist
\item
  Не так страшен чёрт! Оценки \(\hat{\beta}_j\) обладают наименьшей
  дисперсией среди несмещенных оценок. На доверительных интервалах для
  прогнозов мультиколлинеарность не сказывается.
\item
  Немного пожертвовать несмещенностью, чтобы сильно уменьшить дисперсию
\item
  Мечта: получить больше наблюдений
\end{itemize}

\end{frame}

\begin{frame}{Жертвуем несмещенностью}

В модели \(y_i=\beta_1 + \beta_2 x_i +\beta_3 z_i + \ldots + \e_i\) есть
мультиколлинеарность.

\begin{itemize}
\tightlist
\item
  выкинуть часть регрессоров
\end{itemize}

Жертвуем: знанием выкидываемого коэффициента, несмещенностью оставшихся
коэффициентов.

\begin{itemize}
\tightlist
\item
  использовать МНК со штрафом
\end{itemize}

Жертвуем: несмещенностью коэффициентов, доверительными интервалами.

\end{frame}

\begin{frame}{Упражнение {[}у доски{]}}

\(y_i=\beta_1 + \beta_2 x_i + \beta_3 z_i + \beta_4 w_i + \e_i\)

\(R^2_2=0.5\), \(R^2_3=0.95\), \(R^2_4=0.98\)

\begin{itemize}
\tightlist
\item
  Рассчитайте коэффициенты вздутия дисперсии
\item
  Имеет ли место нестрогая мультиколлинеарность
\item
  Между какими переменными есть существенная линейная зависимость?
\end{itemize}

\end{frame}

\begin{frame}{МНК со штрафом}

Общая идея МНК со штрафом:

\begin{itemize}
\tightlist
\item
  Обычный МНК
\end{itemize}

\[
RSS \to \min
\]

\begin{itemize}
\tightlist
\item
  МНК со штрафом
\end{itemize}

\[
RSS + \lambda \cdot (\text{суммарный размер всех коэффициентов}) \to \min
\]

\end{frame}

\begin{frame}{Три популярных варианта оштрафовать МНК}

\begin{itemize}
\tightlist
\item
  Ридж-регрессия
\end{itemize}

\[
\min_{\hat{\beta}} \sum_{i=1}^n (y_i-\hat{y}_i)^2 + \lambda \sum_{j=1}^k \hat{\beta}_j^2
\]

\begin{itemize}
\tightlist
\item
  LASSO
\end{itemize}

\[
\min_{\hat{\beta}} \sum_{i=1}^n (y_i-\hat{y}_i)^2 + \lambda \sum_{j=1}^k |\hat{\beta}_j|
\]

\begin{itemize}
\tightlist
\item
  Метод эластичной сети
\end{itemize}

\[
\min_{\hat{\beta}} \sum_{i=1}^n (y_i-\hat{y}_i)^2 + \lambda_1 \sum_{j=1}^k |\hat{\beta}_j| + \lambda_2 \sum_{j=1}^k \hat{\beta}_j^2
\]

\end{frame}

\begin{frame}{Упражнение {[}у доски{]}}

Выведите оценку \(\hat{\beta}_{Ridge}\) в модели
\(y_i=\beta x_i + \varepsilon_i\)

\end{frame}

\begin{frame}{Метод главных компонент}

Идея:

Позволяет уменьшить число переменных, выбрав самые изменчивые.

Подробности:

\begin{itemize}
\item
  Из старых переменных создаются новые переменные.
\item
  Новые переменные (главные компоненты) являются линейными комбинациями
  старых.
\item
  Исходные переменные предварительно центрируются (то есть из каждой
  переменной вычитается её среднее значение).
\end{itemize}

\end{frame}

\begin{frame}{Переход к новым переменным}

Например:

Исходные переменные (центрированные): \(x_1\) и \(x_2\)

Новые переменные (главные компоненты):

\(pc_1=\frac{1}{\sqrt{2}}x_1+\frac{1}{\sqrt{2}}x_2\)

\(pc_2=\frac{1}{2}x_1-\frac{\sqrt{3}}{2}x_2\).

Сумма квадратов весов, с которыми исходные переменные входят в каждую
новую, равна 1.

\end{frame}

\begin{frame}{Новые переменные}

\begin{itemize}
\tightlist
\item
  \(pc_1\) имеет максимальную выборочную дисперсию \(sVar(pc_1)\)
\item
  \(pc_2\) некоррелирована с \(pc_1\) и имеет максимальную
  \(sVar(pc_2)\)
\item
  \(pc_3\) некоррелирована с \(pc_1\), \(pc_2\) и имеет максимальную
  \(sVar(pc_3)\)
\item
  \ldots{}
\end{itemize}

\end{frame}

\begin{frame}{Игрушечный пример для пояснения идеи}

\begin{tabular}{cc}
Биология & Математика \\ 
\hline 
4 & 5 \\ 
4 & 2 \\ 
4 & 5 \\ 
4 & 4 \\ 
4 & 3 \\ 
4 & 4 \\ 
3 & 3 \\ 
5 & 3 \\ 
\end{tabular}

Упрощенно:

Первая главная компонента --- математика

Вторая главная компонента --- биология

\end{frame}

\begin{frame}{Упражнение {[}у доски{]}}

Найдите первую главную компоненту

\begin{tabular}{cc}
$a_1$ & $a_2$ \\ 
\hline 
2 & 5 \\ 
4 & 1 \\ 
0 & 3 \\ 
\end{tabular}

Не забываем центрировать!

\end{frame}

\begin{frame}{Свойства главных компонент}

\(pc_1=v_{11} \cdot x_1 + v_{21} \cdot x_2 + \ldots + v_{k1} \cdot x_k\)

\ldots{}

\(pc_k=v_{1k} \cdot x_1 + v_{2k} \cdot x_2 + \ldots + v_{kk} \cdot x_k\)

\[
sCorr(pc_j,pc_m)=0
\]

\[
sVar(x_1)+ sVar(x_2) + \ldots + sVar(x_k) =
sVar(pc_1)+ sVar(pc_2) + \ldots + sVar(pc_k)
\]

\end{frame}

\begin{frame}{Немного про линейную алгебру главных компонент}

Если: все переменные центрированы, \(\bar{x}_j=0\)

То: \(pc_j=X \cdot v_j\) и \(|pc_j|^2=\lambda_j\), где

\(\lambda_j\) --- собственные числа, а \(v_{j}\) --- собственные вектора
матрицы \(X'X\)

\end{frame}

\begin{frame}{Что дают главные компоненты?}

\begin{itemize}
\tightlist
\item
  визуализировать сложный набор данных
\item
  увидеть самые информативные переменные
\item
  увидеть особенные наблюдения
\item
  переход к некоррелированным переменным
\end{itemize}

\end{frame}

\begin{frame}{Подводные камни на практике}

\begin{itemize}
\tightlist
\item
  разные единицы измерения
\item
  применение перед регрессией
\end{itemize}

\end{frame}

\begin{frame}{Разные единицы измерения}

Первая главная компонента ``поймает'' переменную с самыми мелкими
единицами измерения.

Вместо самой информативной переменной первой компонентой станет самая
шумная.

Решение:

Нормировать переменные перед применением метода главных компонент:

\(x_j=\frac{a_j-\bar{a}_j}{sd(a_j)}\)

\end{frame}

\begin{frame}{Применение перед регрессией}

Очень часто строят регрессию на несколько первых главных компонент,
например на \(pc_1\), \(pc_2\).

Осторожно:

Хорошо объясняющая переменная может быть почти постоянной.

Подход применим, но надо быть уверенным, что сильная изменчивость
регрессора статистически связана с зависимой переменной.

\end{frame}

\begin{frame}{Метод главных компонент}

\begin{itemize}
\tightlist
\item
  прежде всего полезен сам по себе (!)
\item
  иногда используется для борьбы с мультиколлинеарностью
\end{itemize}

\end{frame}

\begin{frame}{Мораль - мультиколлинеарность}

\begin{itemize}
\item
  Мультиколлинеарность --- нестрогая линейная зависимость между
  регрессорами
\item
  Основное последствие: высокие стандартные ошибки. Следовательно,
  широкие доверительные интервалы для коэффициентов.
\item
  Либо не бороться, либо жертвовать несмещенностью.
\item
  LASSO, Ridge --- два варианта МНК со штрафом
\end{itemize}

\end{frame}

\begin{frame}{Источники мудрости:}

\begin{itemize}
\item
  Артамонов Н.В., Введение в эконометрику: глава 2.9
\item
  Борзых Д.А., Демешев Б.Б. Эконометрика в задачах и упражнениях: глава
  7
\item
  Катышев П.К., Пересецкий А. А. Эконометрика. Начальный курс: главы 4.1
\end{itemize}

\end{frame}

\end{document}
