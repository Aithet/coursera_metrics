\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,landscape]{geometry}
\begin{document}

{\Huge


Лекция 4. Мультиколлинеарность.


\begin{enumerate}
\item Мультиколлинеарность --- наличие линейной зависимости между регрессорами.

\begin{enumerate}
\item строгая (идеальная линейная зависимость) 
\item нестрогая (примерная линейная зависимость)
\end{enumerate}

\item Строгая мультиколлинеарность

Пример:

\[
X=\begin{pmatrix}
1 & 4 & 12 & 8 \\ 
1 & 3 & 3 & 3 \\ 
1 & 1 & 7 & 4 \\ 
\vdots & \vdots & \vdots & \vdots
\end{pmatrix} 
\]

Здесь: $x_{\cdot 2}+x_{\cdot 3}=2x_{\cdot 4}$

\item Строгая мультиколлинеарность 

Частая причина: неправильно включены дамми-переменные

Пример с ошибкой: 

\[
wage_i=\beta_1 + \beta_2 male_i + \beta_3 female_i + \beta_4 educ_i + \varepsilon_i
\]

Здесь:  $x_{\cdot 1}=x_{\cdot 2}+x_{\cdot 3}$

\[
X=\begin{pmatrix}
1 & 1 & 0 & 16 \\ 
1 & 1 & 0 & 11 \\ 
1 & 0 & 1 & 18 \\ 
\vdots & \vdots & \vdots & \vdots
\end{pmatrix} 
\]


\item Последствия строгой мультиколлинеарности

в теории: оценки МНК неединственны

\[
\widehat{wage}_i=15 + 3 male_i -2 female_i + 3 educ_i 
\]

\[
\widehat{wage}_i=28 -10 male_i -15 female_i + 3 educ_i 
\]

\[
\widehat{wage}_i=18 + 0 male_i -5 female_i + 3 educ_i 
\]



\item на практике: 
\begin{enumerate}
\item сообщение об ошибке
\item автоматическое удаление переменной (\verb|R|)
\end{enumerate}

\item Нестрогая мультиколлинеарность

Причина: 

\begin{enumerate}
\item регрессоры, измеряющие примерно одно и то же: валютный курс на начало и на конец дня
\item естественные соотношения между регрессорами: возраст, стаж и количество лет обучения
\end{enumerate}

\item последствия нестрогой мультиколлинеарности

нестрогая мультиколлинеарность НЕ нарушает стандартный набор предпосылок

оценки $\hat{\beta}_j$ несмещенные, асимптотически нормальные, можно проверять гипотезы и строить доверительные интервалы

\item последствия 

один из регрессоров хорошо объясняется другими регрессорами

\[
se(\hat{\beta}_j)=\frac{\hat{\sigma}^2}{RSS_j}=\frac{\hat{\sigma}^2}{TSS_j\cdot (1-R^2_j)}=
\frac{1}{1-R^2_j}\frac{\hat{\sigma}^2}{TSS_j}
\]

высокие стандартные ошибки $se(\hat{\beta}_j)$

\item неприятные проявление высоких стандартных ошибок

\begin{enumerate}
\item очень широкие доверительные интервалы
\item незначимые коэффициенты
\item чувствительность модели к добавлению/удалению наблюдения
\end{enumerate}

\item Типичное проявление

Несколько коэффициентов незначимы по отдельности

Гипотеза об их одновременном равенстве нулю отвергается.

\item количественные признаки

\begin{enumerate}
\item коэффициент вздутия дисперсии (Variance Inflation Factor)

\[
VIF_j=\frac{1}{1-R^2_j}
\]

\[
se(\hat{\beta}_j)=VIF_j  \frac{\hat{\sigma}^2}{TSS_j}
\]

\item выборочные корреляции между регрессорами
\end{enumerate}

Некоторые источники: $VIF_j > 10$, $\widehat{Corr}(x_{\cdot j},x_{\cdot m})>0.9$

\item Что делать?

\begin{enumerate}
\item Не так страшен чёрт! Оценки $\hat{\beta}_j$ обладают наименьшей дисперсией среди несмещенных оценок. На доверительных интервалах для прогнозов мультиколлинеарность не сказывается.
\item Пожертвовать несмещенностью
\item Мечта: получить больше наблюдений 
\end{enumerate}

\item Жертвуем несмещенностью

Модель зависит от всех регрессоров!

\begin{enumerate}
\item выкинуть часть регрессоров

Жертвуем: знанием коэффициента, несмещенностью коэффициентов

\item использовать МНК со штрафом 

Жертвуем: несмещенностью коэффициентов, доверительными интервалами
\end{enumerate}

Жертвуем несмещенностью!

\item упражнение у чудо доски:

$R^2_2=0.5$, $R^2_3=0.95$, $R^2_4=0.98$

Рассчитайте $VIF_j$, между какими переменными есть линейная зависимость?

\item МНК со штрафом 

\begin{enumerate}
\item Ридж-регрессия

\[
\min_{\hat{\beta}} \sum_{i=1}^n (y_i-\hat{y}_i)^2 + \lambda \sum_{j=1}^k \hat{\beta}_j^2
\]

\item LASSO

\[
\min_{\hat{\beta}} \sum_{i=1}^n (y_i-\hat{y}_i)^2 + \lambda \sum_{j=1}^k |\hat{\beta}_j|
\]

\item Метод эластичной сети

\[
\min_{\hat{\beta}} \sum_{i=1}^n (y_i-\hat{y}_i)^2 + \lambda_1 \sum_{j=1}^k |\hat{\beta}_j| + \lambda_2 \sum_{j=1}^k \hat{\beta}_j^2
\]


\end{enumerate}

\item чудо-доска, упражнение

Выведите оценку $\hat{\beta}_{Ridge}$ в модели $y_i=\beta x_i + \varepsilon_i$

\item метод главных компонент

Позволяет уменьшить число переменных, выбрав самые изменчивые 

\item  переход к новым переменным

Например:

Исходные переменные (центрированные): $x_1$ и $x_2$

Новые переменные (главные компоненты): 

$pc_1=\frac{1}{\sqrt{2}}x_1+\frac{1}{\sqrt{2}}x_2$ 

$pc_2=\frac{1}{2}x_1-\frac{\sqrt{3}}{2}x_2$.

Сумма квадратов весов равна 1.

\item Новые переменные

\begin{enumerate}
\item $pc_1$ имеет максимальную выборочную дисперсию $\widehat{Var}(pc_1)$
\item $pc_2$ некоррелирована с $pc_1$ и имеет максимальную $\widehat{Var}(pc_2)$
\item $pc_3$ некоррелирована с $pc_1$, $pc_2$ и имеет максимальную $\widehat{Var}(pc_3)$
\item ...
\end{enumerate}


\item игрушечный пример для пояснения идеи

\begin{tabular}{cc}
Биология & Математика \\ 
\hline 
4 & 5 \\ 
4 & 2 \\ 
4 & 5 \\ 
4 & 4 \\ 
4 & 3 \\ 
4 & 4 \\ 
3 & 3 \\ 
5 & 3 \\ 
\end{tabular} 

Первая главная компонента --- математика

Вторая главная компонента --- биология

\item чудо-доска

Найдите первую главную компоненту

\begin{tabular}{cc}
$a_1$ & $a_2$ \\ 
\hline 
2 & 5 \\ 
4 & 1 \\ 
0 & 3 \\ 
\end{tabular} 

Не забываем центировать! 

\item Свойства главных компонент

$pc_1=v_{11} \cdot x_1 +  v_{21} \cdot x_2 + \ldots + v_{k1} \cdot x_k$

...

$pc_k=v_{1k} \cdot x_1 +  v_{2k} \cdot x_2 + \ldots + v_{kk} \cdot x_k$


\[
\widehat{Corr}(pc_j,pc_m)=0
\]
 
\[
\widehat{Var}(x_1)+ \widehat{Var}(x_2) + \ldots + \widehat{Var}(x_k) =
\widehat{Var}(pc_1)+ \widehat{Var}(pc_2) + \ldots + \widehat{Var}(pc_k)
\]



\item Вставка с линейной алгеброй

Если: все переменные центрированы, $\bar{x}_j=0$

То: $pc_j=X \cdot v_j$ и $|pc_j|^2=\lambda_j$, где

$\lambda_j$ --- собственные числа, а $v_{j}$ --- собственные вектора матрицы $X'X$

\item Что дают главные компоненты?

\begin{enumerate}
\item визуализировать сложный набор данных
\item увидеть самые информативные переменные
\item увидеть особенные наблюдения
\item переход к некоррелированным переменным
\end{enumerate}

\item Подводные камни на практике

\begin{enumerate}
\item разные единицы измерения 
\item применение перед регрессией
\end{enumerate}

\item Разные единицы измерения

первая главная компонента <<поймает>> переменную с самыми мелкими единицами измерения

вместо самой информативной --- самая шумная

нормировать переменные $x_j=\frac{a_j-\bar{a}_j}{se(a_l)}$

\item Применение перед регрессией

строят регрессию на несколько первых главных компонент, например на $pc_1$, $pc_2$

Осторожно: 

хорошо объясняющая переменная может быть почти постоянной

\item Метод главных компонент

\begin{enumerate}
\item полезен сам по себе 
\item иногда используется для борьбы с мультиколлинеарностью
\end{enumerate}

\item Мораль - мультиколлинеарность

\begin{enumerate}
\item зависимость между регрессорами

\item высокие стандартные ошибки

\item либо не бороться, либо жертвовать несмещенностью
\end{enumerate}


\end{enumerate}

} % end Huge

\end{document}