\documentclass[ignorenonframetext,]{beamer}
\usetheme{CambridgeUS}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\usepackage{lmodern}
\ifxetex
  \usepackage{fontspec,xltxtra,xunicode}
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\else
  \ifluatex
    \usepackage{fontspec}
    \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
    \newcommand{\euro}{€}
  \else
    \usepackage[T1]{fontenc}
    \usepackage[utf8]{inputenc}
      \fi
\fi
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}

% Comment these out if you don't want a slide with just the
% part/section/subsection/subsubsection title:
\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \let\insertsectionnumber\relax
  \let\sectionname\relax
  \frame{\sectionpage}
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
\usepackage[russian]{babel}

\title{Эконометрика. Лекция 8. Модели временных рядов}

\begin{document}
\frame{\titlepage}

\begin{frame}{Временные ряды:}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Многомерные
\end{itemize}

(тут табличка)

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Одномерные
\end{itemize}

\end{frame}

\begin{frame}{Одномерный временной ряд}

Временной ряд --- последовательность случайных величин

\[
y_1, y_2, y_3, \ldots
\]

\end{frame}

\begin{frame}{Без предположений невозможно прогнозировать}

1, 2, 3, 4, 5, ?

(потом появляется правильный ответ: 42)

\end{frame}

\begin{frame}{Базовое предположение --- стационарность}

Временной ряд называется стационарным, если:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $E(y_1)=E(y_2)=E(y_3)=\ldots$
\item
  $Var(y_1)=Var(y_2)=Var(y_3)=\ldots=\gamma_0$
\item
  $Cov(y_1,y_2)=Cov(y_2,y_3)=Cov(y_3,y_4)=\ldots=\gamma_1$
\item
  $Cov(y_1,y_3)=Cov(y_2,y_4)=Cov(y_3,y_5)=\ldots=\gamma_2$
\item
  \ldots
\end{itemize}

\end{frame}

\begin{frame}{Предпосылки коротко:}

Временной ряд называется стационарным, если:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $E(y_t)=const$
\item
  $Cov(y_t,y_{t-k})=\gamma_k$
\end{itemize}

\end{frame}

\begin{frame}{Автоковариационная функция}

$\gamma_k=Cov(y_t, y_{t-k})$ --- (авто)-ковариационная функция процесса

\end{frame}

\begin{frame}{Самый простой пример --- белый шум}

Ряд $\varepsilon_t$ --- белый шум, если:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $E(\varepsilon_t)=0$
\item
  $Var(\varepsilon_t)=\sigma^2$
\item
  $Cov(\varepsilon_t,\varepsilon_{t-k})=0$
\end{itemize}

\end{frame}

\begin{frame}{Пример белого шума}

$\varepsilon_t \sim N(0,4)$ и независимы

(график)

\end{frame}

\begin{frame}{Конвенция об обозначениях}

На эту лекцию $\varepsilon_t$ всегда обозначает белый шум!

\end{frame}

\begin{frame}{Примеры нестационарного процесса}

\begin{itemize}
\item
  Процесс с детерминистическим трендом
\item
  Случайное блуждание
\end{itemize}

\end{frame}

\begin{frame}{Процесс с детерминистическим трендом}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $y_t= 5 + 6t + \varepsilon_t$.
\end{itemize}

(тут график)

Здесь: $E(y_t)=5+6t\neq const$

\end{frame}

\begin{frame}{Случайное блуждание}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $\begin{cases} y_0 = 0 \\ y_t = y_{t-1} + 2 + \varepsilon_t \end{cases}$
\end{itemize}

(тут график)

Здесь: \$Var(y\_t)=t \sigma\^{}2 \$

\end{frame}

\begin{frame}{Процесс скользящего среднего}

Процесс представимый в виде

\[
y_t= \mu + \varepsilon_t + a_1 \varepsilon_{t-1} + \ldots a_q \varepsilon_{t-q}
\]

\end{frame}

\begin{frame}{Обозначение процесса скользящего среднего}

$y_t \sim MA(q)$, Moving Average

\end{frame}

\begin{frame}{Чудо доска. Пример MA процесса}

\[
y_t = 5 + \varepsilon_t + 3 \varepsilon_{t-1} -2\varepsilon_{t-2}
\]

Найдите $E(y_t)$, $Var(y_t)$, $Cov(y_t,y_{t-k})$

\end{frame}

\begin{frame}{Запись с помощью оператора лага}

$L$ --- оператор лага:

\begin{itemize}
\item
  $Ly_t=y_{t-1}$
\item
  $L^2y_t=y_{t-2}$
\item
  \ldots
\end{itemize}

\end{frame}

\begin{frame}{Пример записи с помощью оператора лага}

$MA(2):$

\[
y_t= 2 + \varepsilon_t + 3\varepsilon_{t-1}-2\varepsilon_{t-2}
\]

\[
y_t= 2 +(1+3L-2L^2)\varepsilon_t
\]

\end{frame}

\begin{frame}{Интерпретация:}

Коэффициенты плохо интерпретируемы

У стационарного процесса:

$\rho_k=Corr(y_t, y_{t-k})$ --- (авто)-корреляционная функция процесса

\end{frame}

\begin{frame}{Интерпретация}

Если $y_t$ --- стационарный процесс и $y_t \sim N(\mu_y, \sigma^2_y)$,
то:

$\rho_k$ --- на сколько в среднем изменится $y_{t}$ при росте $y_{t-k}$
на единицу

\end{frame}

\begin{frame}{Чудо доска. Автокорреляционная функция MA процесса}

\[
y_t = 5 + \varepsilon_t + 3 \varepsilon_{t-1} -2\varepsilon_{t-2}
\]

\end{frame}

\begin{frame}{Частная автокорреляционная функция-идея}

$\phi_k$ --- частная автокорреляционная функция

$\rho_k$ --- автокорреляционная функция

(тут картинка со стрелочками)

\end{frame}

\begin{frame}{Частная автокорреляционная функция-интерпретация}

Если $y_t$ --- стационарный процесс и $y_t \sim N(\mu_y, \sigma^2_y)$,
то:

$\phi_k$ --- на сколько в среднем изменится $y_{t}$ при росте $y_{t-k}$
на единицу

при фиксированных $y_{t-1}$, $y_{t-2}$, \ldots, $y_{t-k+1}$

\end{frame}

\begin{frame}{Частная автокорреляционная функция-определение}

\[
\phi_{k}=Cor(y_t - P(y_t), y_{t-k} - P(y_{t-k}))
\]

где $P(y_t)$ --- проекция случайной величины $y_t$ на линейную оболочку
величин $y_{t_1}$, $y_{t-2}$, \ldots, $y_{t-k+1}$.

\end{frame}

\begin{frame}{Частная автокорреляция алгоритм подсчёта}

\[
\gamma_0 \phi_1 = \gamma_1  
\]

\[
\begin{cases}
\gamma_0 *_1 + \gamma_1 \phi_2  = \gamma_1 \\
\gamma_1 *_1 + \gamma_0 \phi_2  = \gamma_2 
\end{cases}
\]

\[
\begin{cases}
\gamma_0 *_1 + \gamma_1 *_2 + \gamma_2 \phi_3 = \gamma_1 \\
\gamma_1 *_1 + \gamma_0 *_2 + \gamma_1 \phi_3 = \gamma_2 \\
\gamma_2 *_1 + \gamma_1 *_2 + \gamma_0 \phi_3 = \gamma_3
\end{cases}
\]

\ldots

Прим. для монтажа: уравнения имеет смысл выводить по очереди (убирать
предыдущее, когда следущее появилось)

\end{frame}

\begin{frame}{Чудо доска. Частная Автокорреляционная функция MA
процесса}

\[
y_t = 5 + \varepsilon_t + 3 \varepsilon_{t-1} -2\varepsilon_{t-2}
\]

\end{frame}

\begin{frame}{Процесс авторегрессии}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Стационарный процесс вида \[
  y_t=c + b_1 y_{t-1} + b_2 y_{t-2} + \ldots + b_p y_{t-p} + \varepsilon_t
  \]
\end{itemize}

\end{frame}

\begin{frame}{Обозначение процесса авторегрессии}

$y_t \sim AR(p)$, AutoRegression

\end{frame}

\begin{frame}{Чудо-доска. Частная и обычная автокорреляционные функции
для AR процесса}

\[
y_t = 2 + 0.5 y_{t-1} + \varepsilon_t \; \varepsilon_t \sim N(0,\sigma^2)
\]

Найдите $\rho_k$, $\phi_k$

\end{frame}

\begin{frame}{Альтернативная форма записи}

\[
y_t = 2 + 0.5 y_{t-1} + \varepsilon_t
\]

Или

\[
(y_t - 4) = 0.5 (y_{t-1} - 4) + \varepsilon_t
\]

\end{frame}

\begin{frame}{Важное предупрежедение}

Из одного уравнения \$y\_t = 2 + 0.5 y\_\{t-1\} + \varepsilon\_t \$ не
следует автоматически стационарность (!)

\end{frame}

\begin{frame}{Чудо-доска. Пример множества решений}

\[
y_t = 2 + 0.5 y_{t-1} + \varepsilon_t \; \varepsilon_t \sim N(0,1)
\]

\begin{itemize}
\item
  $y_0=0$, $y_1\sim N(2,1)$, $y_2\sim N(3, 1.25)$, \ldots
\item
  $y_0\sim N(3, 4/3)$, $y_1\sim N(3, 4/3)$, $y_2\sim N(3, 4/3)$, \ldots
\end{itemize}

\end{frame}

\begin{frame}{Подразумеваем стационарное решение}

Пишем: \[
y_t = 2 + 0.5 y_{t-1} + \varepsilon_t \; \varepsilon_t \sim N(0,1)
\]

Подразумеваем:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $y_0\sim N(3, 4/3)$, $y_1\sim N(3, 4/3)$, $y_2\sim N(3, 4/3)$, \ldots
\end{itemize}

\end{frame}

\begin{frame}{AR процесс можно записать с помощью лага}

\[
y_t=2+0.5y_{t-1}-0.06y_{t-2}+\varepsilon_t
\]

\[
(1-0.5L+0.06L^2)y_t=2+\varepsilon_t
\]

\end{frame}

\begin{frame}{Характеристический многочлен}

\[
(1-0.5L+0.06L^2)y_t=2+\varepsilon_t
\]

\[
f(L)y_t=2+\varepsilon_t
\]

$f(L)$ --- характеристический многочлен

\end{frame}

\begin{frame}{Когда у есть стационарное решение?}

\[
f(L)y_t=c+\varepsilon_t
\]

Если корни характеристического уравнения AR процесса по модулю больше
единицы, то существует единственное стационарное решение, в котором
$y_t$ выражается через прошлые шумы, то есть через $\varepsilon_t$,
$\varepsilon_{t-1}$, $\varepsilon_{t-2}$, \ldots

\end{frame}

\begin{frame}{Чудо-доска}

Два примера AR(2): стационарный и нет

\end{frame}

\begin{frame}{Прогнозирование}

Прогноз на $h$ шагов вперед: $E(y_{t+h}|y_t, y_{t-1}, y_{t-2}, \ldots)$

Часто кратко обозначают: $\hat{y}_{t+h}$

\end{frame}

\begin{frame}{Чудо-доска}

\[
y_t=2+0.5y_{t-1}-0.06y_{t-2}+\varepsilon_t \; \varepsilon_t \sim N(0;4)
\]

$y_{100}=4$, $y_{99}=3$.

постройте точечный и интервальный прогноз на 1 и 2 шага вперед

\end{frame}

\begin{frame}{Модель авторегрессии и скользящего среднего}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Стационарный процесс вида

  \begin{multline}
  \nonumber
  y_t=c + b_1 y_{t-1} + b_2 y_{t-2} + \ldots + b_p y_{t-p} + \\
  \varepsilon_t + a_1 \varepsilon_{t-1} + \ldots + a_q \varepsilon_{t-q}
  \end{multline}
\end{itemize}

где сумма $p+q$ минимально возможна

\end{frame}

\begin{frame}{Обозначение}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $y_t \sim ARMA(p,q)$
\end{itemize}

\end{frame}

\begin{frame}{Сумма $p+q$ минимально возможная}

\begin{itemize}
\item
  $y_t=\varepsilon_t$
\item
  $y_t-y_{t-1}=\varepsilon_t-\varepsilon_{t-1}$
\end{itemize}

Вывод: $y_t \sim ARMA(0,0)$

\end{frame}

\begin{frame}{ARMA --- это наше всё!}

Теорема. Любой стационарный процесс можно представить в виде
$AR(\infty)$

Вывод. С помощью $ARMA(p,q)$ можно компактно и очень точно описать любой
стационарный процесс

\end{frame}

\begin{frame}{Итого про ARMA(p,q)}

\begin{itemize}
\item
  коэффициенты не интерпретируемы
\item
  используются для прогнозирования
\end{itemize}

\end{frame}

\begin{frame}{Оценивание коэффициентов}

Есть $T$ наблюдений: $y_1$, $y_2$, $y_3$, \ldots, $y_T$

Чаще всего используется метод максимального правдоподобия

\end{frame}

\begin{frame}{Подробности метода максимального правдоподобия}

\begin{itemize}
\item
  Предполагается независимость и нормальность
  $\varepsilon_t \sim N(0;\sigma^2)$
\item
  Стационарность $y_t$
\end{itemize}

\begin{multline}
\nonumber
y_t=c + b_1 y_{t-1} + b_2 y_{t-2} + \ldots + b_p y_{t-p} + \\
\varepsilon_t + a_1 \varepsilon_{t-1} + \ldots + a_q \varepsilon_{t-q}
\end{multline}

\end{frame}

\begin{frame}{Результат метода максимального правдоподобия}

На выходе получаем оценки

\[
\hat{\theta}=(\hat{a}_1, \ldots, \hat{a}_q, \hat{b}_1, \ldots, \hat{b}_q, \hat{\sigma}^2)
\]

И оценку их ковариационной матрицы $\widehat{Var}(\hat{\theta})$

\end{frame}

\begin{frame}{Проверка гипотез и доверительные интервалы}

\[
\frac{\hat{a}_j - a_j}{se(\hat{a}_j)} \to N(0;1)
\]

\end{frame}

\begin{frame}{Выборочная автокорреляционная функция}

ACF --- autocorrelation function

\[
\hat{\rho}_k = \frac{\sum_{t=1}^{T} (y_t-\bar{y})(y_{t-k}-\bar{y})}{\sum_{t=1}^{T} (y_t-\bar{y})^2}
\]

\end{frame}

\begin{frame}{Выборочная частная автокорреляционная функция}

PACF --- partial autocorrelation function

Получим $\hat{\phi}_k$ из оценки регрессии

\[
\hat{y}_t = * + * \cdot y_{t-1} + *  \cdot y_{t_2} + \ldots + *  \cdot y _{t-k+1} + \phi_k y_{t-k} + u_t
\]

\end{frame}

\begin{frame}{Примечания к расчету автокорреляционной функции}

\begin{itemize}
\item
  Для оценки каждого $\hat{\phi}_k$ строится отдельная регрессия
\item
  Из каждой регрессии нужен только последний коэффициент
\end{itemize}

\end{frame}

\begin{frame}{Алгоритм на практике}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Графики ряда, ACF, PACF
\item
  Если ряд нестационарный, то преобразуем
\item
  Выбираем $p$ и $q$
\item
  Оцениваем $ARMA(p,q)$
\item
  Прогнозируем
\end{enumerate}

\end{frame}

\begin{frame}{Основное преобразование}

Взятие разности: переход от $y_t$ к $\Delta y_t$

\end{frame}

\begin{frame}{Обозначение}

\begin{itemize}
\item
  $y_t \sim ARIMA(p,1,q)$ равносильно $\Delta y_t \sim ARMA(p,q)$
\item
  $y_t \sim ARIMA(p,0,q)$ равносильно $y_t \sim ARMA(p,q)$
\end{itemize}

\end{frame}

\begin{frame}{Выбор $p$ и $q$ по графикам}

График выборочной корреляционной функции есть даже у нестационарного
процесса!

\end{frame}

\begin{frame}{Белый шум}

(график)

\end{frame}

\begin{frame}{Случайное блуждание (нестационарный процесс!)}

(график)

\end{frame}

\begin{frame}{Процесс с трендом (нестационарный процесс!)}

(график)

\end{frame}

\begin{frame}{AR(1) и AR(2)}

два графика

\end{frame}

\begin{frame}{MA(1) и MA(2)}

два графика

\end{frame}

\begin{frame}{ARMA(1,1)}

график

\end{frame}

\begin{frame}{Мораль}

\begin{itemize}
\item
  временные ряды: стационарные и нет
\item
  для стационарных --- модель ARMA
\end{itemize}

\end{frame}

\end{document}
