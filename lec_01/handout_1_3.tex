\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={handout\_1 2 3},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

%%% Change title format to be more compact
\usepackage{titling}
\setlength{\droptitle}{-2em}
  \title{handout\_1 2 3}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{}
  \preauthor{}\postauthor{}
  \date{}
  \predate{}\postdate{}


\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}


\begin{document}

\maketitle


{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{3}
\tableofcontents
}
\section{Лекция 1. Метод наименьших квадратов без
статистики}\label{-1.-----}

\section{Лекция 2.}\label{-2.}

\section{Лекция 3.}\label{-3.}

\subsubsection{Геометрическая иллюстрация МНК}\label{--}

Тут картинка.

\subsubsection{Вывод формулы для оценок коэффициентов}\label{----}

Если вектор $y$ перпендикулярен вектору $x$, то их скалярное
произведение должно быть равно нулю, т.к. $cos(90^\circ)=0$, a скалярное
произведение равно:

\[
(x,y)=|x|\cdot  |y| \cdot cos(x,y)
\]

С другой стороны, скалярное произведение равно

\[
(x,y)=x_1 y_1+x_2 y_2 + \ldots + x_n y_n = \sum_{i=1}^n x_i y_i = x'y
\]

Значит условие перпендикулярности векторов $x$ и $y$ можно кратко
записать как $x'y=0$. Столбца матрицы регрессоров $X$ ортогональны
остаткам регрессии, вектору $\hat{\varepsilon}$:

\[
X'\hat{\varepsilon}=0
\]

Заметим, что здесь 0 --- это вектор размера $k\times 1$. Подставляем
формулу для остатков, $\hat{\varepsilon}=y-X'\hat{\beta}$:

\[
X'(y-X\hat{\beta})=0
\]

Раскрываем скобки и переносим в разные стороны уравнения:

\[
X'y-X'X\hat{\beta}=0
\]

\[
X'X\hat{\beta}=X'y
\]

Матрица $X'$ имеет размер $k\times n$, поэтому на неё сокращать нельзя.
Хотя иногда хочется :) А вот обратная матрица к матрице $X'X$
существует, если среди столбцов $X$ нет линейно зависимых и $n\geq k$.
Домножаем обе части уравнения слева на $(X'X)^{-1}$:

\[
\hat{\beta}=(X'X)^{-1}X'y
\]

Ура! Мы получили формулу для МНК-оценок множественной регрессии!
Заметьте, что она подозрительно похожа на формулу МНК-оценки для случая
одного оцениваемого параметра. В модели $y_i=\beta x_i +\varepsilon_i$
МНК-оценка коэффициента $\beta$ имела вид
$\hat{\beta}=\sum x_i y_i /\sum x_i^2$.

\subsubsection{Матрица-шляпница}\label{-}

Если $\hat{\beta}=(X'X)^{-1}X'y$, то вектор прогнозов, $\hat{y}$, будет
равен $\hat{y}=X\hat{\beta}=X(X'X)^{-1}X'y$. Матрицу $H=X(X'X)^{-1}X'$
по-английски называют ``hat-matrix'', матрицей-шляпницей, потому, что
она надевает на $y$ шляпку: $\hat{y}=H\cdot y$. Умножение любого вектора
на матрицу $H$ проецирует этот вектор на пространство, порождаемое
регрессорами. Поскольку сами регрессоры уже лежат в этом пространстве,
то $H\cdot X=X$. Матрица $H$ идемпотентная, то есть возведенная в
произвольную натуральную степень даст саму себя, $H^n=H$. В этом легко
можно убедиться либо перемножив руками $H$ на $H$, \[
H\cdot H=X(X'X)^{-1}X'X(X'X)^{-1}X'=X(X'X)^{-1}X'=H
\] либо из геометрических соображений. Умножение на $H$ несколько раз
подряд, это проецирование результата проецирования. А проекция от
проекции совпадает с проекцией.

Собственными числами матрицы $H$ могут быть только нули или единицы.
Действительно, при проецировании часть векторов сохраняются (те, что
лежали в пространстве регрессоров), часть превращается в ноль (те, что
были ортогональны пространству регрессоров), а все другие при
проецировании меняют направление.

Ранг матрицы-шляпницы можно посчитать, воспользовавшись тем, что
$rk (AB)=rk (BA)$:

\[
rk(X(X'X)^{-1}X')=rk(X'X(X'X)^{-1})=rk(I_{k\times k})=k
\]

\subsubsection{Математическое ожидание и ковариационная
матрица}\label{----}

Если $y=(y_1, \ldots, y_n)'$ --- случайный вектор, то для него
определены математическое ожидание, $E(y)$, и ковариационная матрица,
$Var(y)$.

Определение. Если $y$ --- вектор-столбец случайных величин,

\[
y=
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
,
\text{ то }
E(y)=
\begin{pmatrix}
E(y_1) \\
E(y_2) \\
\vdots \\
E(y_n)
\end{pmatrix}
\]

Определение. $Var(y)=E(yy')-E(y)E(y')$.

Согласно определению ковариационной матрицы:

\[
Var(y) = \begin{pmatrix}
Var(y_1) & Cov(y_1,y_2) & \cdots & Cov(y_1,y_n) \\
Cov(y_2,y_1) & Var(y_2) & \cdots & Cov(y_2,y_n) \\
Cov(y_3,y_1) & Cov(y_3,y_2) & \cdots & Cov(y_3,y_n) \\
\vdots & \vdots & \vdots & \vdots \\
Cov(y_n,y_1) & Cov(y_n,y_2) & \cdots & Var(y_n) \\
\end{pmatrix}
\]

Определение. $Cov(y,z)=E(yz')-E(y)E(z')$

Свойства:

Если $A$ и $B$ --- неслучайные матрицы, $a$ и $b$ --- неслучайные
вектора, $y$ и $w$ --- случайные вектора подходящих размеров, все
математические ожидания и ковариационные матрицы существуют, то:

$E(a)=a$

$E(Ay+b)=AE(y)+b$ и $E(yA+b)=E(y)A+b$

$E(y+w)=E(y)+E(w)$

$Var(a)=0$

$Var(Ay+b)=AVar(y)A'$

$Var(y+z)=Var(y)+Var(z)+Cov(y,z)+Cov(z,y)$

$Cov(Ay+a,Bz+b)=ACov(y,z)B'$

$Cov(y,z)=Cov(z,y)'$

$Cov(y+z,w)=Cov(y,w)+Cov(z,w)$ и $Cov(y,z+w)=Cov(y,z)+Cov(y,w)$

$Cov(y,y)=Var(y)$

\subsubsection{Статистические свойства МНК оценок}\label{---}

Если:

\begin{enumerate}
\item Истинная зависимость имеет вид $y_i=\beta_1 + \beta_2 x_{i2} + \ldots + \beta_k x_{ik}+\varepsilon_i$

В матричном виде: $y=X\beta + \varepsilon$
\item С помощью МНК оценивается регрессия $y$ на константу, $x_{.2}$, $x_{.3}$, \ldots, $x_{.k}$

В матричном виде: $\hat{\beta}=(X'X)^{-1}X'y$
\item Наблюдений больше, чем оцениваемых коэффициентов $\beta$: $n>k$
\item Строгая экзогенность: $E(\varepsilon_i | \text{ все } x_{ij})=0$

В матричном виде: $E(\varepsilon_i | X)=0$
\item Условная гомоскедастичность: $E(\varepsilon_i^2 | \text{ все } x_{ij})=\sigma^2$

В матричном виде: $E(\varepsilon_i^2 | X)=\sigma^2$
\item  $Cov(\varepsilon_i,\varepsilon_j | X)=0$ при $i \neq j$
\item  вектора $(x_{i.},y_i)$ --- независимы и одинаково распределены
\item  с вероятностью 1 среди регрессоров нет линейно зависимых
$rank(X)=k$
$det(X'X)\neq 0$
$(X'X)^{-1}$ существует
\end{enumerate}

То (свойства для конечных выборок, не требующие нормальности
$\varepsilon$):

\begin{enumerate}
\item [тГМ] МНК оценки $\hat{\beta}$ линены по $y$:
$\hat{\beta_j}=c_1 y_1 + ... + c_n y_n$
\item  [тГМ] $E(\hat{\beta} |X )=\beta$, и в частности $E(\hat{\beta})=\beta$
\item  [тГМ] Для любой альтернативной оценки $\hat{\beta}^{alt}$ удовлетворяющей свойствам 1 и 2:
$Var(\hat{\beta}_j^{alt} | X)\geq Var(\hat{\beta}_j | X)$
$Var(\hat{\beta}_j^{alt} )\geq Var(\hat{\beta}_j )$
\item  $Var(\hat{\beta} | X )=\sigma^2 (X'X)^{-1}$
\item $Cov(\hat{\beta},\hat{\varepsilon} | X)=0$
\item  $E(\hat{\sigma}^2 |X ) = \sigma^2$, и $E(\hat{\sigma}^2 ) = \sigma^2$ ?остается ли при условной ГК?
\end{enumerate}

свойства для конечных выборок, требующие нормальности $\varepsilon$ Если
дополнительно известно, что $\varepsilon |X \sim N$, (в частности
$\varepsilon$ и $X$ независимы) то:

\begin{enumerate}
\item $t|X \sim t_{n-k}$, $t\sim t_{n-k}$
\item  $RSS/\sigma^2 |X \sim \chi^2_{n-k}$, $RSS/\sigma^2 \sim \chi^2_{n-k}$
\item $F$ тест $F|X \sim F$
\end{enumerate}

Асимптотические свойства:

\begin{enumerate}
\item  $\hat{\beta} \to \beta$ по вероятности

\item $t \to N(0,1)$
\item  $rF \to \chi^2_r$, $r$ --- число ограничений
\item  $nR^2 \to \chi^2_{k-1}$
$\frac{RSS}{n-k} \to \sigma^2 $
\end{enumerate}

\section{Лекция 4. Мультиколлинеарность. Метод главных
компонент.}\label{-4.-.---.}

\section{Лекция 5. Гетероскедастичность.}\label{-5.-.}

\section{Лекция 6. Автокорреляция.}\label{-6.-.}

\section{Лекция 7. Метод максимального правдоподобия. Логит и
пробит-модели.}\label{-7.---.----.}

\section{Лекция 8. Процессы авторегрессии и скользящего
среднего}\label{-8.-----}

\section{Лекция 9. Инструментальные переменные}\label{-9.--}

\section{Лекция 10. Квантильная регрессия. Случайный лес. Байесовский
подход.}\label{-10.--.--.--.}

\end{document}
