---
title: "handout_1 2 3"
output:
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes
    toc: yes
    toc_depth: 3
---



Тут картинка.


Столбца матрицы регрессоров $X$ ортогональны остаткам регрессии, вектору $\hat{\varepsilon}$:

\[
X'\hat{\varepsilon}=0
\]

Заметим, что здесь 0 --- это вектор размера $k\times 1$. Подставляем формулу для остатков, $\hat{\varepsilon}=y-X'\hat{\beta}$:

\[
X'(y-X\hat{\beta})=0
\]

Раскрываем скобки и переносим в разные стороны уравнения:


\[
X'y-X'X\hat{\beta}=0
\]

\[
X'X\hat{\beta}=X'y
\]

Матрица $X'$ имеет размер $k\times n$, поэтому на неё сокращать нельзя. Хотя иногда хочется :) А вот обратная матрица к матрице $X'X$ существует, если среди столбцов $X$ нет линейно зависимых и $n\geq k$. Домножаем обе части уравнения слева на $(X'X)^{-1}$:

\[
\hat{\beta}=(X'X)^{-1}X'y
\]

Ура! Мы получили формулу для МНК-оценок множественной регрессии! Заметьте, что она подозрительно похожа на формулу МНК-оценки для случая одного оцениваемого параметра. В модели $y_i=\beta x_i +\varepsilon_i$ МНК-оценка коэффициента $\beta$ имела вид $\hat{\beta}=\sum x_i y_i /\sum x_i^2$. 

### Матрица-шляпница

Если $\hat{\beta}=(X'X)^{-1}X'y$, то вектор прогнозов, $\hat{y}$, будет равен $\hat{y}=X\hat{\beta}=X(X'X)^{-1}X'y$. Матрицу $H=X(X'X)^{-1}X'$ по-английски называют "hat-matrix", матрицей-шляпницей, потому, что она надевает на $y$ шляпку: $\hat{y}=H\cdot y$. Умножение любого вектора на матрицу $H$ проецирует этот вектор на пространство, порождаемое регрессорами. Поскольку сами регрессоры уже лежат в этом пространстве, то $H\cdot X=X$. Матрица $H$ идемпотентная, то есть возведенная в произвольную натуральную степень даст саму себя, $H^n=H$. В этом легко можно убедиться либо перемножив руками $H$ на $H$, 
\[
H\cdot H=X(X'X)^{-1}X'X(X'X)^{-1}X'=X(X'X)^{-1}X'=H
\]
либо из геометрических соображений. Умножение на $H$ несколько раз подряд, это проецирование результата проецирования. А проекция от проекции совпадает с проекцией. 

Собственными числами матрицы $H$ могут быть только нули или единицы. Действительно, при проецировании часть векторов сохраняются (те, что лежали в пространстве регрессоров), часть превращается в ноль (те, что были ортогональны пространству регрессоров), а все другие при проецировании меняют направление.

Ранг матрицы-шляпницы можно посчитать, воспользовавшись тем, что $rk (AB)=rk (BA)$:

\[
rk(X(X'X)^{-1}X')=rk(X'X(X'X)^{-1})=rk(I_{k\times k})=k
\]



### Математическое ожидание и ковариационная матрица

Если $y=(y_1, \ldots, y_n)'$ --- случайный вектор, то для него определены математическое ожидание, $E(y)$, и ковариационная матрица, $Var(y)$.

Определение. Если $y$ --- вектор-столбец случайных величин, 



\[
y=
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
,
\text{ то }
E(y)=
\begin{pmatrix}
E(y_1) \\
E(y_2) \\
\vdots \\
E(y_n)
\end{pmatrix}
\]


Определение. $Var(y)=E(yy')-E(y)E(y')$. 

Согласно определению ковариационной матрицы:

\[
Var(y) = \begin{pmatrix}
Var(y_1) & Cov(y_1,y_2) & \cdots & Cov(y_1,y_n) \\
Cov(y_2,y_1) & Var(y_2) & \cdots & Cov(y_2,y_n) \\
Cov(y_3,y_1) & Cov(y_3,y_2) & \cdots & Cov(y_3,y_n) \\
\vdots & \vdots & \vdots & \vdots \\
Cov(y_n,y_1) & Cov(y_n,y_2) & \cdots & Var(y_n) \\
\end{pmatrix}
\]


Определение. $Cov(y,z)=E(yz')-E(y)E(z')$



Свойства:

Если $A$ и $B$ --- неслучайные матрицы, $a$ и $b$ --- неслучайные вектора, $y$ и $w$ --- случайные вектора подходящих размеров, все математические ожидания и ковариационные матрицы существуют, то:


$E(a)=a$

$E(Ay+b)=AE(y)+b$ и $E(yA+b)=E(y)A+b$

$E(y+w)=E(y)+E(w)$

$Var(a)=0$

$Var(Ay+b)=AVar(y)A'$

$Var(y+z)=Var(y)+Var(z)+Cov(y,z)+Cov(z,y)$

$Cov(Ay+a,Bz+b)=ACov(y,z)B'$

$Cov(y,z)=Cov(z,y)'$

$Cov(y+z,w)=Cov(y,w)+Cov(z,w)$ и $Cov(y,z+w)=Cov(y,z)+Cov(y,w)$

$Cov(y,y)=Var(y)$






